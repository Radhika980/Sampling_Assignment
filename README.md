# Machine Learning Models

**1. Random Forest**: An ensemble model made up of multiple decision trees. Each tree is trained on a random subset of the data, and the final prediction is made by combining the predictions of all the trees.

**2. Support Vector Machines (SVMs)**: A method for separating data points into different classes using a hyperplane in a high-dimensional space. 

**3. K-Nearest Neighbors (KNN)**: A simple model that classifies new data points based on the classes of their nearest neighbors in the feature space. 

**4. Logistic Regression**: A statistical method used for analyzing a dataset in which there are one or more independent variables that determine an outcome.

**5. Decision Trees**: A tree-structured model that splits the dataset into smaller subsets based on the most important features. 

# Sampling techniques

**1. Simple Random Sampling**: A basic sampling technique where each data point in the dataset has an equal probability of being selected in the sample.

**2. Stratified Sampling**: A sampling technique where the population is divided into subgroups (strata) based on a specific characteristic, and samples are taken from each stratum in proportion to the population.

**3. Cluster Sampling**: This involves dividing the population into clusters, or groups, and then randomly selecting a few clusters for sampling. This method can be useful when the population is large and spread out geographically.

**4. Systematic Sampling**: This involves selecting data points at fixed intervals from a larger population. This method is useful when the data is ordered, and can be faster and more efficient than random sampling.

**5. Convenience sampling**: This is a non-probability sampling technique in which the researcher selects the most easily accessible individuals or data points from the population. 




