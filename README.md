# Machine Learning Models

**1. Random Forest**: An ensemble model made up of multiple decision trees. Each tree is trained on a random subset of the data, and the final prediction is made by combining the predictions of all the trees.

**2. Support Vector Machines (SVMs)**: A method for separating data points into different classes using a hyperplane in a high-dimensional space. 

**3. K-Nearest Neighbors (KNN)**: A simple model that classifies new data points based on the classes of their nearest neighbors in the feature space. 

**4. Logistic Regression**: A statistical method used for analyzing a dataset in which there are one or more independent variables that determine an outcome.

**5. Decision Trees**: A tree-structured model that splits the dataset into smaller subsets based on the most important features. 

# Sampling techniques

**1. Simple Random Sampling**: A basic sampling technique where each data point in the dataset has an equal probability of being selected in the sample.

**2. Stratified Sampling**: A sampling technique where the population is divided into subgroups (strata) based on a specific characteristic, and samples are taken from each stratum in proportion to the population.

**3. Cluster Sampling**: This involves dividing the population into clusters, or groups, and then randomly selecting a few clusters for sampling. This method can be useful when the population is large and spread out geographically.

**4. Systematic Sampling**: This involves selecting data points at fixed intervals from a larger population. This method is useful when the data is ordered, and can be faster and more efficient than random sampling.

**5. Convenience sampling**: This is a non-probability sampling technique in which the researcher selects the most easily accessible individuals or data points from the population. 

# Comparison Table

The table below shows the accuracies of each sampling technique on five different machine learning models. The dataset used for all models is a balanced version of the original unbalanced dataset.

![image](https://user-images.githubusercontent.com/73791285/219968793-e37dc031-6c67-4b5e-9edd-225063abde78.png)

After applying the TOPSIS method to evaluate the performance of different machine learning models, we have identified the best performing model as the "Random Forest" algorithm. The TOPSIS method helped us to select the most appropriate model based on different sampling techniques.

![WhatsApp Image 2023-02-20 at 12 30 40 AM](https://user-images.githubusercontent.com/73791285/219969335-520f044c-4505-4adb-a7d7-934aeb3c8a59.jpeg)


**Random Forest** is an ensemble model that uses multiple decision trees to make predictions. It is a highly flexible and powerful machine learning algorithm that can be used for both classification and regression problems. Random Forest is known for its high accuracy, robustness to noise and overfitting, and ability to handle large datasets with high-dimensional feature spaces.

The selection of Random Forest as the best model indicates that it outperformed other models that were evaluated in terms of their overall performance on the given dataset. Therefore, this model is likely to be the most effective for making predictions on new data. However, it is important to note that the choice of the best model may vary depending on the dataset and the specific problem being solved, and it is always recommended to use a range of evaluation techniques to validate the results.





